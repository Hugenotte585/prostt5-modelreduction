{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProstT5 \n",
    "\n",
    "The model can be found in [HuggingFace](https://huggingface.co/Rostlab/ProstT5) with some initial code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Feature Extraction code from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add .. to path\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from src.uniprot import download_fasta_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/prostt5-modelreduction/libs/ProstT5/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False) #.to(device) - the tokenizer is not a pytorch object and cannot be loaded to the device\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Rostlab/ProstT5\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.full() if device=='cpu' else model.half()\n",
    "\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from libs/ProstT5/notebooks/ProstT5_inverseFolding.ipynb\n",
    "#@title Read in file in FASTA format. { display-mode: \"form\" }\n",
    "def read_fasta( in_path, is_3Di ):\n",
    "    '''\n",
    "        Reads in fasta file containing a single or multiple sequences.\n",
    "        Returns dictionary.\n",
    "    '''\n",
    "\n",
    "    sequences = dict()\n",
    "    with open( in_path, 'r' ) as fasta_f:\n",
    "        for line in fasta_f:\n",
    "            # get uniprot ID from header and create new entry\n",
    "            if line.startswith('>'):\n",
    "                # starts with P and is 6 characters long\n",
    "                # get index of first P\n",
    "                uniprot_id = line[line.find('P'):line.find('P')+6]\n",
    "                sequences[ uniprot_id ] = ''\n",
    "            else:\n",
    "                # repl. all whie-space chars and join seqs spanning multiple lines\n",
    "                if is_3Di:\n",
    "                    sequences[ uniprot_id ] += ''.join( line.split() ).replace(\"-\",\"\").lower() # drop gaps and cast to lower-case\n",
    "                else:\n",
    "                    sequences[ uniprot_id ] += ''.join( line.split() ).replace(\"-\",\"\")\n",
    "                    \n",
    "\n",
    "    example = sequences[uniprot_id]\n",
    "\n",
    "    print(\"##########################\")\n",
    "    print(f\"Input is 3Di: {is_3Di}\")\n",
    "    print(f\"Example sequence: >{uniprot_id}\\n{example}\")\n",
    "    print(\"##########################\")\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download random fastas from uniprot to get the sequences\n",
    "uniprot_ids = [f\"P{str(i).zfill(5)}\" for i in range(12345, 12366)]\n",
    "fasta_list, failed_ids = download_fasta_parallel(uniprot_ids, num_proc=8, save=True, save_dir=\"../../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Input is 3Di: False\n",
      "Example sequence: >P12365\n",
      "MDPYKHRPSSAFNAPYWTTNSGAPVWNNDSSLTVGARGPILLEDYHCEKLANFDRERIPERVVHARGASAKGFFEVTHDITHLTCADFLRAPGVQTPVIVRFSTVIHERGSPETLRDPRGFAVKFYTREGNWDLVGNNFPVFFIRDGIKFPDMVHALKPNPRTHIQDNWRILDFFSHHPESLHMFSFLFDDVGIPADYRHMDGSGVHTYTLVSRAGTVTYVKFHWRPTCGVRSLMDDEAVRCGANHSHATKDLTDAIAAGNFPEWTLYIQTMDPEMEDRLDDLDPLDVTKTWPEDTFPLQPVGRLVLNRNIDNFFAENEQLAFCPGLIVPGIYYSDDKLLQTRIFSYSDTQRHRLGPNYLLLPANAPKCAHHNNHYDGSMNFMHRHEEVDYFPSRYDAVRNAPRYPIPTAHIAGRREKTVISKENNFKQPGERYRAMDPARQERFITRWVDALSDPRLTHEIRTIWLSNWSQADRSLGQKLASRLSAKPSM\n",
      "##########################\n",
      "S G G K K I K V D K P L G L G G G L T V D I D A\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "fasta_file = glob.glob(\"../../data/2024*.fasta\")[0]\n",
    "sequences = read_fasta(fasta_file, is_3Di=False)\n",
    "sequences = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for _,sequence in sequences.items()]\n",
    "sequences = sorted(sequences, key=len)\n",
    "print(sequences[0])\n",
    "sequences = [ \"<AA2fold>\" + \" \" + s if s.isupper() else \"<fold2AA>\" + \" \" + s # this expects 3Di sequences to be already lower-case\n",
    "                      for s in sequences\n",
    "                    ]\n",
    "# only use the shortest sequence for now\n",
    "sequences = sequences[:1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24\n"
     ]
    }
   ],
   "source": [
    "min_len = min([ len(\"\".join(s.removeprefix(\"<AA2fold> \").split())) for s in sequences])\n",
    "max_len = max([ len(\"\".join(s.removeprefix(\"<AA2fold>\").split())) for s in sequences])\n",
    "print(min_len, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.batch_encode_plus(sequences,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Generation configuration for \"folding\" (AA-->3Di)\n",
    "gen_kwargs_aa2fold = {\n",
    "                  \"do_sample\": True,\n",
    "                  \"num_beams\": 3, \n",
    "                  \"top_p\" : 0.95, \n",
    "                  \"temperature\" : 1.2, \n",
    "                  \"top_k\" : 6,\n",
    "                  \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from AA to 3Di (AA-->3Di)\n",
    "translations=[]\n",
    "with torch.no_grad():\n",
    "  translations = model.generate( \n",
    "              ids.input_ids, \n",
    "              attention_mask=ids.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              early_stopping=True, # stop early if end-of-text token is generated\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_aa2fold\n",
    "  )\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_translations = tokenizer.batch_decode( translations, skip_special_tokens=True )\n",
    "structure_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_translations ] # predicted 3Di strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_examples_backtranslation = [ \"<fold2AA>\" + \" \" + s for s in decoded_translations]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids_backtranslation = tokenizer.batch_encode_plus(sequence_examples_backtranslation,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Example generation configuration for \"inverse folding\" (3Di-->AA)\n",
    "gen_kwargs_fold2AA = {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\" : 0.85,\n",
    "            \"temperature\" : 1.0,\n",
    "            \"top_k\" : 3,\n",
    "            \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from 3Di to AA (3Di-->AA)\n",
    "with torch.no_grad():\n",
    "  backtranslations = model.generate( \n",
    "              ids_backtranslation.input_ids, \n",
    "              attention_mask=ids_backtranslation.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              #early_stopping=True, # stop early if end-of-text token is generated; only needed for beam-search\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_fold2AA\n",
    "  )\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_backtranslations = tokenizer.batch_decode( backtranslations, skip_special_tokens=True )\n",
    "aminoAcid_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_backtranslations ] # predicted amino acid strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before translation:\n",
      "SGGKKIKVDKPLGLGGGLTVDIDA\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "sequence = sequences[0]\n",
    "sequence = sequence.removeprefix(\"<AA2fold> \")\n",
    "# remove whitespace \n",
    "sequence = \"\".join(sequence.split())\n",
    "print(\"Before translation:\")\n",
    "print(sequence)\n",
    "print(len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After translation:\n",
      "ddwdwdwdwddppppdidididt\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(\"After translation:\")\n",
    "print(structure_sequences[0])\n",
    "print(len(structure_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back translation:\n",
      "MATSIKLVFRNDGNNQWHYEIIP\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(\"Back translation:\")\n",
    "print(aminoAcid_sequences[0])\n",
    "print(len(aminoAcid_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use backtranslation to generate structure -> compare structure to original structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
